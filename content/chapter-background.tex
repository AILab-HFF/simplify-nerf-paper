% !TEX root = ../main.tex
%
\chapter{Background}
\label{sec:background}

\section{View Synthesis}

View synthesis is a process used in computer graphics and computer vision that involves creating new, synthetic images of a scene from viewpoints that were not directly captured by a camera.
This technique leverages existing images and, often, some form of geometric information about the scene to interpolate or extrapolate visual data, enabling the generation of perspective-correct views at desired locations.
The goal of view synthesis is to produce realistic and accurate representations of a three-dimensional scene from novel viewpoints, enhancing applications such as virtual reality (VR), augmented reality (AR), 3D television, and film production.
The following sections provide an overview of traditional view synthesis techniques.

\paragraph{Image-Based Rendering Techniques}
Image-based rendering (IBR) techniques focus on synthesizing new views of a scene using pre-captured images, relying minimally on geometric models.
The core premise of IBR is to directly utilize the radiance information captured in these images, manipulating and interpolating it to generate new viewpoints without the need for detailed 3D reconstruction.
This approach is particularly advantageous in scenarios where rapid rendering is necessary or when accurate and comprehensive geometric data is unavailable.
IBR methods are characterized by their ability to deliver highly photorealistic results, as they inherently capture lighting, shadows, and reflections true to the captured scene.
These methods, such as those using light fields or lumigraphs, excel in applications where visual realism and viewer immersion are critical, such as virtual reality and digital cinematography, by efficiently simulating the complexity of real-world lighting and textures \cite{buehler_unstructured_2001,chen_view_1993,debevec_modeling_1996,gortler_lumigraph_1996,levoy_light_1996}.

\paragraph{Volumetric Methods}
Volumetric rendering techniques construct a three-dimensional volume of the scene, often represented as a grid of voxels, each containing data such as color and opacity which contribute to the final image through a process akin to 3D texturing.
Unlike surface-based modeling which requires explicit surfaces, volumetric methods fill the entire data space, allowing for the handling of complex phenomena like fog, clouds, and fire, which do not have clear boundaries.
This approach is advantageous when the scene involves intricate details and volumetric phenomena that traditional polygon-based rendering might struggle to capture accurately.
Volumetric methods often use techniques like voxel coloring to integrate multiple images into a cohesive 3D model, providing a basis for generating realistic and detailed views from any angle, crucial for applications like medical imaging, scientific visualization, and comprehensive architectural walkthroughs \cite{curless_volumetric_1996,seitz_photorealistic_1999}.

\section{Neural Approaches to View Synthesis}
Neural approaches to view synthesis have significantly advanced the capabilities of generating realistic views from sparse data.
These methods integrate deep learning techniques to enhance the synthesis process, offering substantial improvements over traditional geometric and image-based methods.

\paragraph{Deep Learning for View Synthesis}
Deep learning models, particularly Convolutional Neural Networks (CNNs), are employed to predict depth and color information from sparse sets of images.
These models facilitate the synthesis of intermediate views by interpolating between captured viewpoints, efficiently handling scenarios with incomplete data where traditional methods struggle \cite{kalantari_learning-based_2016,maxim_tatarchenko_single-view_2015,peter_hedman_deep_2019}.

\paragraph{Learning Volumetric Representations}
Recent advances in neural view synthesis have explored volumetric representations to encode three-dimensional scene information effectively.
This approach leverages deep learning to construct volumetric grids or embeddings that capture both color and spatial data, allowing for dynamic and realistic rendering of new views.
Such volumetric techniques demonstrate a significant departure from traditional modeling by providing a framework where 3D features are embedded directly into the network, enabling sophisticated scene understanding and rendering without explicit geometric reconstruction \cite{lombardi_neural_2019,sitzmann_deepvoxels_2019}.

\paragraph{Multiplane Images and Scene Representation}
Multiplane Images (MPIs) represent a neural approach to view synthesis where scenes are decomposed into layers at different depths.
Neural networks learn to create these layered representations from a limited number of views.
The learned MPIs can then be efficiently re-rendered from new perspectives using traditional graphics techniques.
This method provides a robust solution for generating photorealistic and geometrically coherent views by accommodating variations in scene complexity and handling occlusions effectively \cite{mildenhall_local_2019,penner_soft_2017,pratul_p_srinivasan_pushing_2019,tao_zhou_stereo_2018}.

\paragraph{End-to-End Learning}
End-to-end learning with deep neural networks has significantly simplified the view synthesis process by enabling a single network to handle multiple synthesis tasks concurrently.
This approach reduces the complexity and potential failure points associated with multi-stage synthesis methods.
By training on large datasets of posed imagery, these networks learn complex mappings from input images to synthesized views, efficiently handling challenging scenarios \cite{chen_photographic_2017,flynn_deep_2016}.


\section{Neural Radiance Fields}