
@software{noauthor_automatic1111_2022,
	title = {{AUTOMATIC}1111 - Stable Diffusion Web {UI}},
	rights = {{AGPL}-3.0},
	url = {https://github.com/AUTOMATIC1111/stable-diffusion-webui},
	abstract = {Stable Diffusion web {UI}},
	urldate = {2023-10-16},
	date = {2022-08},
	note = {original-date: 2022-08-22T14:05:26Z},
}

@online{team_gradio_nodate,
	title = {Gradio},
	url = {https://gradio.app},
	abstract = {Build \& Share Delightful Machine Learning Apps},
	author = {Team, Gradio},
	urldate = {2023-10-16},
	langid = {german},
	file = {Snapshot:/Users/eduard.von-briesen/Zotero/storage/N2SXMK6R/www.gradio.app.html:text/html},
}

@inproceedings{wang_clip-nerf_2022,
	title = {{CLIP}-{NeRF}: Text-and-Image Driven Manipulation of Neural Radiance Fields},
	pages = {3835--3844},
	booktitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Wang, Can and Chai, Menglei and He, Mingming and Chen, Dongdong and Liao, Jing},
	date = {2022-06},
	file = {Wang et al. - 2022 - CLIP-NeRF Text-and-Image Driven Manipulation of N.pdf:/Users/eduard.von-briesen/Zotero/storage/QSE9GGYZ/Wang et al. - 2022 - CLIP-NeRF Text-and-Image Driven Manipulation of N.pdf:application/pdf},
}

@misc{bar-tal_text2live_2022,
	title = {Text2LIVE: Text-Driven Layered Image and Video Editing},
	author = {Bar-Tal, Omer and Ofri-Amar, Dolev and Fridman, Rafail and Kasten, Yoni and Dekel, Tali},
	date = {2022},
	note = {\_eprint: 2204.02491},
	file = {Bar-Tal et al. - 2022 - Text2LIVE Text-Driven Layered Image and Video Edi.pdf:/Users/eduard.von-briesen/Zotero/storage/I3G8Z7US/Bar-Tal et al. - 2022 - Text2LIVE Text-Driven Layered Image and Video Edi.pdf:application/pdf},
}

@article{muller_instant_2022,
	title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
	volume = {41},
	url = {https://doi.org/10.1145/3528223.3530127},
	doi = {10.1145/3528223.3530127},
	pages = {102:1--102:15},
	number = {4},
	journaltitle = {{ACM} Trans. Graph.},
	author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
	date = {2022-07},
	note = {Place: New York, {NY}, {USA}
Publisher: {ACM}},
	file = {Müller et al. - 2022 - Instant Neural Graphics Primitives with a Multires.pdf:/Users/eduard.von-briesen/Zotero/storage/NP73B3FX/Müller et al. - 2022 - Instant Neural Graphics Primitives with a Multires.pdf:application/pdf},
}

@inproceedings{bao_sine_2023,
	title = {{SINE}: Semantic-driven Image-based {NeRF} Editing with Prior-guided Editing Field},
	booktitle = {The {IEEE}/{CVF} Computer Vision and Pattern Recognition Conference ({CVPR})},
	author = {Bao, Chong and Zhang, Yinda and Yang, Bangbang and Fan, Tianxing and Yang, Zesong and Bao, Hujun and Zhang, Guofeng and Cui, Zhaopeng},
	date = {2023},
	file = {Bao et al. - 2023 - SINE Semantic-driven Image-based NeRF Editing wit.pdf:/Users/eduard.von-briesen/Zotero/storage/WTEP74IW/Bao et al. - 2023 - SINE Semantic-driven Image-based NeRF Editing wit.pdf:application/pdf},
}

@misc{wu_palettenerf_2022,
	title = {{PaletteNeRF}: Palette-based Color Editing for {NeRFs}},
	author = {Wu, Qiling and Tan, Jianchao and Xu, Kun},
	date = {2022},
	note = {\_eprint: 2212.12871},
	file = {Wu et al. - 2022 - PaletteNeRF Palette-based Color Editing for NeRFs.pdf:/Users/eduard.von-briesen/Zotero/storage/B75YG3CP/Wu et al. - 2022 - PaletteNeRF Palette-based Color Editing for NeRFs.pdf:application/pdf},
}

@inproceedings{haque_instruct-nerf2nerf_2023,
	title = {Instruct-{NeRF}2NeRF: Editing 3D Scenes with Instructions},
	booktitle = {Proceedings of the {IEEE}/{CVF} International Conference on Computer Vision},
	author = {Haque, Ayaan and Tancik, Matthew and Efros, Alexei and Holynski, Aleksander and Kanazawa, Angjoo},
	date = {2023},
	file = {Haque et al. - 2023 - Instruct-NeRF2NeRF Editing 3D Scenes with Instruc.pdf:/Users/eduard.von-briesen/Zotero/storage/UABJT6NX/Haque et al. - 2023 - Instruct-NeRF2NeRF Editing 3D Scenes with Instruc.pdf:application/pdf},
}

@inproceedings{yuan_nerf-editing_2022,
	title = {{NeRF}-Editing: Geometry Editing of Neural Radiance Fields},
	pages = {18353--18364},
	booktitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Yuan, Yu-Jie and Sun, Yang-Tian and Lai, Yu-Kun and Ma, Yuewen and Jia, Rongfei and Gao, Lin},
	date = {2022-06},
	file = {Yuan et al. - 2022 - NeRF-Editing Geometry Editing of Neural Radiance .pdf:/Users/eduard.von-briesen/Zotero/storage/WXK4HSYZ/Yuan et al. - 2022 - NeRF-Editing Geometry Editing of Neural Radiance .pdf:application/pdf},
}

@inproceedings{tancik_nerfstudio_2023,
	title = {Nerfstudio: A Modular Framework for Neural Radiance Field Development},
	series = {{SIGGRAPH} '23},
	booktitle = {{ACM} {SIGGRAPH} 2023 Conference Proceedings},
	author = {Tancik, Matthew and Weber, Ethan and Ng, Evonne and Li, Ruilong and Yi, Brent and Kerr, Justin and Wang, Terrance and Kristoffersen, Alexander and Austin, Jake and Salahi, Kamyar and Ahuja, Abhik and {McAllister}, David and Kanazawa, Angjoo},
	date = {2023},
	file = {Tancik et al. - 2023 - Nerfstudio A Modular Framework for Neural Radianc.pdf:/Users/eduard.von-briesen/Zotero/storage/495RZA9X/Tancik et al. - 2023 - Nerfstudio A Modular Framework for Neural Radianc.pdf:application/pdf},
}

@inproceedings{laugwitz_construction_2008,
	title = {Construction and Evaluation of a User Experience Questionnaire},
	volume = {5298},
	isbn = {978-3-540-89349-3},
	doi = {10.1007/978-3-540-89350-9_6},
	abstract = {An end-user questionnaire to measure user experience quickly in a simple and immediate way while covering a preferably comprehensive impression of the product user experience was the goal of the reported construction process. An empirical approach for the item selection was used to ensure practical relevance of items. Usability experts collected terms and statements on user experience and usability, including ‘hard’ as well as ‘soft’ aspects. These statements were consolidated and transformed into a first questionnaire version containing 80 bipolar items. It was used to measure the user experience of software products in several empirical studies. Data were subjected to a factor analysis which resulted in the construction of a 26 item questionnaire including the six factors Attractiveness, Perspicuity, Efficiency, Dependability, Stimulation, and Novelty. Studies conducted for the original German questionnaire and an English version indicate a satisfactory level of reliability and construct validity.},
	eventtitle = {{USAB} 2008},
	pages = {63--76},
	author = {Laugwitz, Bettina and Held, Theo and Schrepp, Martin},
	date = {2008-11-20},
	keywords = {Perceived usability, Questionnaire, Software evaluation, Usability assessment, User experience, User satisfaction},
}

@online{noauthor_trpc_nodate,
	title = {{tRPC} - Move Fast and Break Nothing. End-to-end typesafe {APIs} made easy. {\textbar} {tRPC}},
	url = {https://trpc.io/},
	urldate = {2024-03-25},
	file = {tRPC - Move Fast and Break Nothing. End-to-end typesafe APIs made easy. | tRPC:/Users/eduard.von-briesen/Zotero/storage/QUK4WCCG/trpc.io.html:text/html},
}

@online{noauthor_vite_nodate,
	title = {Vite},
	url = {https://vitejs.dev},
	abstract = {Next Generation Frontend Tooling},
	urldate = {2024-03-25},
	langid = {american},
	file = {Snapshot:/Users/eduard.von-briesen/Zotero/storage/GWUZBRHW/vitejs.dev.html:text/html},
}

@online{noauthor_react_nodate,
	title = {React},
	url = {https://react.dev/},
	abstract = {React is the library for web and native user interfaces. Build user interfaces out of individual pieces called components written in {JavaScript}. React is designed to let you seamlessly combine components written by independent people, teams, and organizations.},
	urldate = {2024-03-25},
	langid = {english},
	file = {Snapshot:/Users/eduard.von-briesen/Zotero/storage/7EKI6DWW/react.dev.html:text/html},
}

@online{noauthor_tailwind_2020,
	title = {Tailwind {CSS} - Rapidly build modern websites without ever leaving your {HTML}.},
	url = {https://tailwindcss.com/},
	abstract = {Tailwind {CSS} is a utility-first {CSS} framework for rapidly building modern websites without ever leaving your {HTML}.},
	urldate = {2024-03-25},
	date = {2020-11-15},
	langid = {english},
}

@online{noauthor_daisyui_nodate,
	title = {{daisyUI} — Tailwind {CSS} Components ( version 4 update is here )},
	url = {https://daisyui.com/},
	abstract = {Best Tailwind Components Library - Free {UI} components for Tailwind {CSS}},
	urldate = {2024-03-25},
	langid = {english},
	file = {Snapshot:/Users/eduard.von-briesen/Zotero/storage/GA2YJUU7/daisyui.com.html:text/html},
}

@software{noauthor_nerfstudio-projectviser_2024,
	title = {nerfstudio-project/viser},
	rights = {Apache-2.0},
	url = {https://github.com/nerfstudio-project/viser},
	abstract = {Web-based 3D visualization + Python},
	publisher = {nerfstudio},
	urldate = {2024-03-25},
	date = {2024-03-25},
	note = {original-date: 2022-11-16T00:13:51Z},
	keywords = {python, visualization, web},
}

@software{yi_brentyityro_2024,
	title = {brentyi/tyro},
	rights = {{MIT}},
	url = {https://github.com/brentyi/tyro},
	abstract = {Zero-effort {CLI} interfaces \& config objects, from types},
	author = {Yi, Brent},
	urldate = {2024-03-31},
	date = {2024-03-26},
	note = {original-date: 2021-10-05T08:54:08Z},
	keywords = {argparse, argument-parsing, dataclasses},
}

@article{mildenhall_nerf_2021,
	title = {{NeRF}: representing scenes as neural radiance fields for view synthesis},
	volume = {65},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3503250},
	doi = {10.1145/3503250},
	shorttitle = {{NeRF}},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, ϕ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.},
	pages = {99--106},
	number = {1},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	urldate = {2024-04-08},
	date = {2021-12-17},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/GM44WRYH/Mildenhall et al. - 2021 - NeRF representing scenes as neural radiance field.pdf:application/pdf},
}

@article{feng_meshnet_2019,
	title = {{MeshNet}: Mesh Neural Network for 3D Shape Representation},
	volume = {33},
	rights = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4840},
	doi = {10.1609/aaai.v33i01.33018279},
	shorttitle = {{MeshNet}},
	abstract = {Mesh is an important and powerful type of data for 3D shapes and widely studied in the field of computer vision and computer graphics. Regarding the task of 3D shape representation, there have been extensive research efforts concentrating on how to represent 3D shapes well using volumetric grid, multi-view and point cloud. However, there is little effort on using mesh data in recent years, due to the complexity and irregularity of mesh data. In this paper, we propose a mesh neural network, named {MeshNet}, to learn 3D shape representation from mesh data. In this method, face-unit and feature splitting are introduced, and a general architecture with available and effective blocks are proposed. In this way, {MeshNet} is able to solve the complexity and irregularity problem of mesh and conduct 3D shape representation well. We have applied the proposed {MeshNet} method in the applications of 3D shape classification and retrieval. Experimental results and comparisons with the state-of-the-art methods demonstrate that the proposed {MeshNet} can achieve satisfying 3D shape classification and retrieval performance, which indicates the effectiveness of the proposed method on 3D shape representation.},
	pages = {8279--8286},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Feng, Yutong and Feng, Yifan and You, Haoxuan and Zhao, Xibin and Gao, Yue},
	urldate = {2024-04-18},
	date = {2019-07-17},
	langid = {english},
	note = {Number: 01},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/CC2CG95W/Feng et al. - 2019 - MeshNet Mesh Neural Network for 3D Shape Represen.pdf:application/pdf},
}

@inproceedings{curless_volumetric_1996,
	location = {New York, {NY}, {USA}},
	title = {A volumetric method for building complex models from range images},
	isbn = {978-0-89791-746-9},
	url = {https://dl.acm.org/doi/10.1145/237170.237269},
	doi = {10.1145/237170.237269},
	series = {{SIGGRAPH} '96},
	abstract = {A number of techniques have been developed for reconstructing surfaces by integrating groups of aligned range images. A desirable set of properties for such algorithms includes: incremental updating, representation of directional uncertainty, the ability to fill gaps in the reconstruction, and robustness in the presence of outliers. Prior algorithms possess subsets of these properties. In this paper, we present a volumetric method for integrating range images that possesses all of these properties. Our volumetric representation consists of a cumulative weighted signed distance function. Working with one range image at a time, we first scan-convert it to a distance function, then combine this with the data already acquired using a simple additive scheme. To achieve space efficiency, we employ a run-length encoding of the volume. To achieve time efficiency, we resample the range image to align with the voxel grid and traverse the range and voxel scanlines synchronously. We generate the final manifold by extracting an isosurface from the volumetric grid. We show that under certain assumptions, this isosurface is optimal in the least squares sense. To fill gaps in the model, we tessellate over the boundaries between regions seen to be empty and regions never observed. Using this method, we are able to integrate a large number of range images (as many as 70) yielding seamless, high-detail models of up to 2.6 million triangles.},
	pages = {303--312},
	booktitle = {Proceedings of the 23rd annual conference on Computer graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Curless, Brian and Levoy, Marc},
	urldate = {2024-04-18},
	date = {1996-08-01},
	keywords = {isosurface extraction, range image integration, surface fitting, three-dimensional shape recovery},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/LZBMG6LE/Curless and Levoy - 1996 - A volumetric method for building complex models fr.pdf:application/pdf},
}

@inproceedings{levoy_light_1996,
	location = {New York, {NY}, {USA}},
	title = {Light field rendering},
	isbn = {978-0-89791-746-9},
	url = {https://dl.acm.org/doi/10.1145/237170.237199},
	doi = {10.1145/237170.237199},
	series = {{SIGGRAPH} '96},
	abstract = {A number of techniques have been proposed for flying through scenes by redisplaying previously rendered or digitized views. Techniques have also been proposed for interpolating between views by warping input images, using depth information or correspondences between multiple images. In this paper, we describe a simple and robust method for generating new views from arbitrary camera positions without depth information or feature matching, simply by combining and resampling the available images. The key to this technique lies in interpreting the input images as 2D slices of a 4D function - the light field. This function completely characterizes the flow of light through unobstructed space in a static scene with fixed illumination. We describe a sampled representation for light fields that allows for both efficient creation and display of inward and outward looking views. We have created light fields from large arrays of both rendered and digitized images. The latter are acquired using a video camera mounted on a computer-controlled gantry. Once a light field has been created, new views may be constructed in real time by extracting slices in appropriate directions. Since the success of the method depends on having a high sample rate, we describe a compression system that is able to compress the light fields we have generated by more than a factor of 100:1 with very little loss of fidelity. We also address the issues of antialiasing during creation, and resampling during slice extraction.},
	pages = {31--42},
	booktitle = {Proceedings of the 23rd annual conference on Computer graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Levoy, Marc and Hanrahan, Pat},
	urldate = {2024-04-19},
	date = {1996-08-01},
	keywords = {epipolar analysis, holographic stereogram, image-based rendering, light field, vector quantization},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/9W52MK9V/Levoy and Hanrahan - 1996 - Light field rendering.pdf:application/pdf},
}

@inproceedings{gortler_lumigraph_1996,
	location = {New York, {NY}, {USA}},
	title = {The lumigraph},
	isbn = {978-0-89791-746-9},
	url = {https://dl.acm.org/doi/10.1145/237170.237200},
	doi = {10.1145/237170.237200},
	series = {{SIGGRAPH} '96},
	abstract = {This paper discusses a new method for capturing the complete appearance of both synthetic and real world objects and scenes, representing this information, and then using this representation to render images of the object from new camera positions. Unlike the shape capture process traditionally used in computer vision and the rendering process traditionally used in computer graphics, our approach does not rely on geometric representations. Instead we sample and reconstruct a 4D function, which we call a Lumigraph. The Lumigraph is a subsetof the complete plenoptic function that describes the flow of light at all positions in all directions. With the Lumigraph, new images of the object can be generated very quickly, independent of the geometric or illumination complexity of the scene or object. The paper discusses a complete working system including the capture of samples, the construction of the Lumigraph, and the subsequent rendering of images from this new representation.},
	pages = {43--54},
	booktitle = {Proceedings of the 23rd annual conference on Computer graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Gortler, Steven J. and Grzeszczuk, Radek and Szeliski, Richard and Cohen, Michael F.},
	urldate = {2024-04-19},
	date = {1996-08-01},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/ZNGGKBML/Gortler et al. - 1996 - The lumigraph.pdf:application/pdf},
}

@article{zitnick_high-quality_2004,
	title = {High-quality video view interpolation using a layered representation},
	volume = {23},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/1015706.1015766},
	doi = {10.1145/1015706.1015766},
	abstract = {The ability to interactively control viewpoint while watching a video is an exciting application of image-based rendering. The goal of our work is to render dynamic scenes with interactive viewpoint control using a relatively small number of video cameras. In this paper, we show how high-quality video-based rendering of dynamic scenes can be accomplished using multiple synchronized video streams combined with novel image-based modeling and rendering algorithms. Once these video streams have been processed, we can synthesize any intermediate view between cameras at any time, with the potential for space-time manipulation.In our approach, we first use a novel color segmentation-based stereo algorithm to generate high-quality photoconsistent correspondences across all camera views. Mattes for areas near depth discontinuities are then automatically extracted to reduce artifacts during view synthesis. Finally, a novel temporal two-layer compressed representation that handles matting is developed for rendering at interactive rates.},
	pages = {600--608},
	number = {3},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Zitnick, C. Lawrence and Kang, Sing Bing and Uyttendaele, Matthew and Winder, Simon and Szeliski, Richard},
	urldate = {2024-04-19},
	date = {2004-08-01},
	keywords = {Computer Vision, Dynamic Scenes, Image-Based Rendering},
}

@inproceedings{debevec_modeling_1996,
	location = {New York, {NY}, {USA}},
	title = {Modeling and rendering architecture from photographs: a hybrid geometry- and image-based approach},
	isbn = {978-0-89791-746-9},
	url = {https://dl.acm.org/doi/10.1145/237170.237191},
	doi = {10.1145/237170.237191},
	series = {{SIGGRAPH} '96},
	shorttitle = {Modeling and rendering architecture from photographs},
	abstract = {We present a new approach for modeling and rendering existing architectural scenes from a sparse set of still photographs. Our modeling approach, which combines both geometry-based and image-based techniques, has two components. The first component is a photogrammetric modeling method which facilitates the recovery of the basic geometry of the photographed scene. Our photogrammetric modeling approach is effective, convenient, and robust because it exploits the constraints that are characteristic of architectural scenes. The second component is a model-based stereo algorithm, which recovers how the real scene deviates from the basic model. By making use of the model, our stereo technique robustly recovers accurate depth from widely-spaced image pairs. Consequently, our approach can model large architectural environments with far fewer photographs than current image-based modeling approaches. For producing renderings, we present view-dependent texture mapping, a method of compositing multiple views of a scene that better simulates geometric detail on basic models. Our approach can be used to recover models for use in either geometry-based or image-based rendering systems. We present results that demonstrate our approach's ability to create realistic renderings of architectural scenes from viewpoints far from the original photographs.},
	pages = {11--20},
	booktitle = {Proceedings of the 23rd annual conference on Computer graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Debevec, Paul E. and Taylor, Camillo J. and Malik, Jitendra},
	urldate = {2024-04-19},
	date = {1996-08-01},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/EP6GGLC3/Debevec et al. - 1996 - Modeling and rendering architecture from photograp.pdf:application/pdf},
}

@inproceedings{buehler_unstructured_2001,
	location = {New York, {NY}, {USA}},
	title = {Unstructured lumigraph rendering},
	isbn = {978-1-58113-374-5},
	url = {https://dl.acm.org/doi/10.1145/383259.383309},
	doi = {10.1145/383259.383309},
	series = {{SIGGRAPH} '01},
	abstract = {We describe an image based rendering approach that generalizes many current image based rendering algorithms, including light field rendering and view-dependent texture mapping. In particular, it allows for lumigraph-style rendering from a set of input cameras in arbitrary configurations (i.e., not restricted to a plane or to any specific manifold). In the case of regular and planar input camera positions, our algorithm reduces to a typical lumigraph approach. When presented with fewer cameras and good approximate geometry, our algorithm behaves like view-dependent texture mapping. The algorithm achieves this flexibility because it is designed to meet a set of specific goals that we describe. We demonstrate this flexibility with a variety of examples.},
	pages = {425--432},
	booktitle = {Proceedings of the 28th annual conference on Computer graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Buehler, Chris and Bosse, Michael and {McMillan}, Leonard and Gortler, Steven and Cohen, Michael},
	urldate = {2024-04-19},
	date = {2001-08-01},
	keywords = {image-based rendering},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/M7T5W9U9/Buehler et al. - 2001 - Unstructured lumigraph rendering.pdf:application/pdf},
}

@article{seitz_photorealistic_1999,
	title = {Photorealistic Scene Reconstruction by Voxel Coloring},
	volume = {35},
	issn = {1573-1405},
	url = {https://doi.org/10.1023/A:1008176507526},
	doi = {10.1023/A:1008176507526},
	abstract = {A novel scene reconstruction technique is presented, different from previous approaches in its ability to cope with large changes in visibility and its modeling of intrinsic scene color and texture information. The method avoids image correspondence problems by working in a discretized scene space whose voxels are traversed in a fixed visibility ordering. This strategy takes full account of occlusions and allows the input cameras to be far apart and widely distributed about the environment. The algorithm identifies a special set of invariant voxels which together form a spatial and photometric reconstruction of the scene, fully consistent with the input images. The approach is evaluated with images from both inward-facing and outward-facing cameras.},
	pages = {151--173},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {International Journal of Computer Vision},
	author = {Seitz, Steven M. and Dyer, Charles R.},
	urldate = {2024-04-19},
	date = {1999-11-01},
	langid = {english},
	keywords = {image correspondence, invariants, multi-baseline stereo, occlusion, photorealism, scene reconstruction, voxel representations},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/95DE3SLT/Seitz and Dyer - 1999 - Photorealistic Scene Reconstruction by Voxel Color.pdf:application/pdf},
}

@inproceedings{chen_view_1993,
	location = {New York, {NY}, {USA}},
	title = {View interpolation for image synthesis},
	isbn = {978-0-89791-601-1},
	url = {https://dl.acm.org/doi/10.1145/166117.166153},
	doi = {10.1145/166117.166153},
	series = {{SIGGRAPH} '93},
	abstract = {Image-space simplifications have been used to accelerate the calculation of computer graphic images since the dawn of visual simulation. Texture mapping has been used to provide a means by which images may themselves be used as display primitives. The work reported by this paper endeavors to carry this concept to its logical extreme by using interpolated images to portray three-dimensional scenes. The special-effects technique of morphing, which combines interpolation of texture maps and their shape, is applied to computing arbitrary intermediate frames from an array of prestored images. If the images are a structured set of views of a 3D object or scene, intermediate frames derived by morphing can be used to approximate intermediate 3D transformations of the object or scene. Using the view interpolation approach to synthesize 3D scenes has two main advantages. First, the 3D representation of the scene may be replaced with images. Second, the image synthesis time is independent of the scene complexity. The correspondence between images, required for the morphing method, can be predetermined automatically using the range data associated with the images. The method is further accelerated by a quadtree decomposition and a view-independent visible priority. Our experiments have shown that the morphing can be performed at interactive rates on today?s high-end personal computers. Potential applications of the method include virtual holograms, a walkthrough in a virtual environment, image-based primitives and incremental rendering. The method also can be used to greatly accelerate the computation of motion blur and soft shadows cast by area light sources.},
	pages = {279--288},
	booktitle = {Proceedings of the 20th annual conference on Computer graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Chen, Shenchang Eric and Williams, Lance},
	urldate = {2024-04-19},
	date = {1993-09-01},
	keywords = {image morphing, incremental rendering, interpolation, motion blur, motion compensation, real-time display, shadow, virtual holography, virtual reality},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/SAPYXF6R/Chen and Williams - 1993 - View interpolation for image synthesis.pdf:application/pdf},
}

@inproceedings{flynn_deep_2016,
	title = {Deep Stereo: Learning to Predict New Views from the World's Imagery},
	url = {https://ieeexplore.ieee.org/document/7780964},
	doi = {10.1109/CVPR.2016.595},
	shorttitle = {Deep Stereo},
	abstract = {Deep networks have recently enjoyed enormous success when applied to recognition and classification problems in computer vision [22, 33], but their use in graphics problems has been limited ([23, 7] are notable recent exceptions). In this work, we present a novel deep architecture that performs new view synthesis directly from pixels, trained from a large number of posed image sets. In contrast to traditional approaches, which consist of multiple complex stages of processing, each of which requires careful tuning and can fail in unexpected ways, our system is trained end-to-end. The pixels from neighboring views of a scene are presented to the network, which then directly produces the pixels of the unseen view. The benefits of our approach include generality (we only require posed image sets and can easily apply our method to different domains), and high quality results on traditionally difficult scenes. We believe this is due to the end-to-end nature of our system, which is able to plausibly generate pixels according to color, depth, and texture priors learnt automatically from the training data. We show view interpolation results on imagery from the {KITTI} dataset [12], from data from [1] as well as on Google Street View images. To our knowledge, our work is the first to apply deep learning to the problem of new view synthesis from sets of real-world, natural imagery.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {5515--5524},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Flynn, John and Neulander, Ivan and Philbin, James and Snavely, Noah},
	urldate = {2024-04-19},
	date = {2016-06},
	note = {{ISSN}: 1063-6919},
	keywords = {Google, Image color analysis, Poles and towers, Shape, Three-dimensional displays, Training, Training data},
	file = {Submitted Version:/Users/eduard.von-briesen/Zotero/storage/2XZPKBIB/Flynn et al. - 2016 - Deep Stereo Learning to Predict New Views from th.pdf:application/pdf},
}

@article{liu_neural_2021,
	title = {Neural actor: neural free-view synthesis of human actors with pose control},
	volume = {40},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3478513.3480528},
	doi = {10.1145/3478513.3480528},
	shorttitle = {Neural actor},
	abstract = {We propose Neural Actor ({NA}), a new method for high-quality synthesis of humans from arbitrary viewpoints and under arbitrary controllable poses. Our method is developed upon recent neural scene representation and rendering works which learn representations of geometry and appearance from only 2D images. While existing works demonstrated compelling rendering of static scenes and playback of dynamic scenes, photo-realistic reconstruction and rendering of humans with neural implicit methods, in particular under user-controlled novel poses, is still difficult. To address this problem, we utilize a coarse body model as a proxy to unwarp the surrounding 3D space into a canonical pose. A neural radiance field learns pose-dependent geometric deformations and pose- and view-dependent appearance effects in the canonical space from multi-view video input. To synthesize novel views of high-fidelity dynamic geometry and appearance, {NA} leverages 2D texture maps defined on the body model as latent variables for predicting residual deformations and the dynamic appearance. Experiments demonstrate that our method achieves better quality than the state-of-the-arts on playback as well as novel pose synthesis, and can even generalize well to new poses that starkly differ from the training poses. Furthermore, our method also supports shape control on the free-view synthesis of human actors.},
	pages = {1--16},
	number = {6},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Liu, Lingjie and Habermann, Marc and Rudnev, Viktor and Sarkar, Kripasindhu and Gu, Jiatao and Theobalt, Christian},
	urldate = {2024-04-22},
	date = {2021-12},
	langid = {english},
	file = {Full Text:/Users/eduard.von-briesen/Zotero/storage/QZESPFEN/Liu et al. - 2021 - Neural actor neural free-view synthesis of human .pdf:application/pdf},
}

@article{kalantari_learning-based_2016,
	title = {Learning-based view synthesis for light field cameras},
	volume = {35},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/2980179.2980251},
	doi = {10.1145/2980179.2980251},
	abstract = {With the introduction of consumer light field cameras, light field imaging has recently become widespread. However, there is an inherent trade-off between the angular and spatial resolution, and thus, these cameras often sparsely sample in either spatial or angular domain. In this paper, we use machine learning to mitigate this trade-off. Specifically, we propose a novel learning-based approach to synthesize new views from a sparse set of input views. We build upon existing view synthesis techniques and break down the process into disparity and color estimation components. We use two sequential convolutional neural networks to model these two components and train both networks simultaneously by minimizing the error between the synthesized and ground truth images. We show the performance of our approach using only four corner sub-aperture views from the light fields captured by the Lytro Illum camera. Experimental results show that our approach synthesizes high-quality images that are superior to the state-of-the-art techniques on a variety of challenging real-world scenes. We believe our method could potentially decrease the required angular resolution of consumer light field cameras, which allows their spatial resolution to increase.},
	pages = {1--10},
	number = {6},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Kalantari, Nima Khademi and Wang, Ting-Chun and Ramamoorthi, Ravi},
	urldate = {2024-04-22},
	date = {2016-11-11},
	langid = {english},
	file = {Submitted Version:/Users/eduard.von-briesen/Zotero/storage/K7JSMJE2/Kalantari et al. - 2016 - Learning-based view synthesis for light field came.pdf:application/pdf},
}

@inproceedings{chen_photographic_2017,
	title = {Photographic Image Synthesis with Cascaded Refinement Networks},
	url = {https://ieeexplore.ieee.org/document/8237430},
	doi = {10.1109/ICCV.2017.168},
	abstract = {We present an approach to synthesizing photographic images conditioned on semantic layouts. Given a semantic label map, our approach produces an image with photographic appearance that conforms to the input layout. The approach thus functions as a rendering engine that takes a two-dimensional semantic specification of the scene and produces a corresponding photographic image. Unlike recent and contemporaneous work, our approach does not rely on adversarial training. We show that photographic images can be synthesized from semantic layouts by a single feedforward network with appropriate structure, trained end-to-end with a direct regression objective. The presented approach scales seamlessly to high resolutions; we demonstrate this by synthesizing photographic images at 2-megapixel resolution, the full resolution of our training data. Extensive perceptual experiments on datasets of outdoor and indoor scenes demonstrate that images synthesized by the presented approach are considerably more realistic than alternative approaches.},
	pages = {1520--1529},
	booktitle = {2017 {IEEE} International Conference on Computer Vision ({ICCV})},
	author = {Chen, Qifeng and Koltun, Vladlen},
	urldate = {2024-04-22},
	date = {2017-10},
	note = {{ISSN}: 2380-7504},
	keywords = {Training, Gallium nitride, Image generation, Image resolution, Layout, Semantics, Solid modeling},
	file = {IEEE Xplore Abstract Record:/Users/eduard.von-briesen/Zotero/storage/W4DLE8XV/8237430.html:text/html;Submitted Version:/Users/eduard.von-briesen/Zotero/storage/XV8C55VY/Chen and Koltun - 2017 - Photographic Image Synthesis with Cascaded Refinem.pdf:application/pdf},
}

@inproceedings{sitzmann_deepvoxels_2019,
	title = {{DeepVoxels}: Learning Persistent 3D Feature Embeddings},
	url = {https://ieeexplore.ieee.org/document/8953309},
	doi = {10.1109/CVPR.2019.00254},
	shorttitle = {{DeepVoxels}},
	abstract = {In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To this end, we propose {DeepVoxels}, a learned representation that encodes the view-dependent appearance of a 3D scene without having to explicitly model its geometry. At its core, our approach is based on a Cartesian 3D grid of persistent embedded features that learn to make use of the underlying 3D scene structure. Our approach combines insights from 3D geometric computer vision with recent advances in learning image-to-image mappings based on adversarial loss functions. {DeepVoxels} is supervised, without requiring a 3D reconstruction of the scene, using a 2D re-rendering loss and enforces perspective and multi-view geometry in a principled manner. We apply our persistent 3D scene representation to the problem of novel view synthesis demonstrating high-quality results for a variety of challenging scenes.},
	eventtitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {2432--2441},
	booktitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Sitzmann, Vincent and Thies, Justus and Heide, Felix and Nießner, Matthias and Wetzstein, Gordon and Zollhöfer, Michael},
	urldate = {2024-04-22},
	date = {2019-06},
	note = {{ISSN}: 2575-7075},
	keywords = {Deep Learning, Image and Video Synthesis},
	file = {IEEE Xplore Abstract Record:/Users/eduard.von-briesen/Zotero/storage/UW6I6SYF/8953309.html:text/html;Submitted Version:/Users/eduard.von-briesen/Zotero/storage/PNE7JANG/Sitzmann et al. - 2019 - DeepVoxels Learning Persistent 3D Feature Embeddi.pdf:application/pdf},
}

@article{mildenhall_local_2019,
	title = {Local light field fusion: practical view synthesis with prescriptive sampling guidelines},
	volume = {38},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3306346.3322980},
	doi = {10.1145/3306346.3322980},
	shorttitle = {Local light field fusion},
	abstract = {We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image ({MPI}) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. In practice, we apply this bound to capture and render views of real world scenes that achieve the perceptual quality of Nyquist rate view sampling while using up to 4000X fewer views. We demonstrate our approach's practicality with an augmented reality smart-phone app that guides users to capture input images of a scene and viewers that enable realtime virtual exploration on desktop and mobile platforms.},
	pages = {1--14},
	number = {4},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Ortiz-Cayon, Rodrigo and Kalantari, Nima Khademi and Ramamoorthi, Ravi and Ng, Ren and Kar, Abhishek},
	urldate = {2024-04-22},
	date = {2019-08-31},
	langid = {english},
	file = {Full Text:/Users/eduard.von-briesen/Zotero/storage/ZE8Q6NRX/Mildenhall et al. - 2019 - Local light field fusion practical view synthesis.pdf:application/pdf},
}

@article{lombardi_neural_2019,
	title = {Neural volumes: learning dynamic renderable volumes from images},
	volume = {38},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3306346.3323020},
	doi = {10.1145/3306346.3323020},
	shorttitle = {Neural volumes},
	abstract = {Modeling and rendering of dynamic scenes is challenging, as natural scenes often contain complex phenomena such as thin structures, evolving topology, translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g., light field video) typically rely on constrained viewing conditions, which limit interactivity. We circumvent these difficulties by presenting a learning-based approach to representing dynamic objects inspired by the integral projection model used in tomographic imaging. The approach is supervised directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two primary components: an encoder-decoder network that transforms input images into a 3D volume representation, and a differentiable ray-marching operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared to screen-space rendering techniques. The encoder-decoder architecture learns a latent representation of a dynamic scene that enables us to produce novel content sequences not seen during training. To overcome memory limitations of voxel-based representations, we learn a dynamic irregular grid structure implemented with a warp field during ray-marching. This structure greatly improves the apparent resolution and reduces grid-like artifacts and jagged motion. Finally, we demonstrate how to incorporate surface-based representations into our volumetric-learning framework for applications where the highest resolution is required, using facial performance capture as a case in point.},
	pages = {1--14},
	number = {4},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Lombardi, Stephen and Simon, Tomas and Saragih, Jason and Schwartz, Gabriel and Lehrmann, Andreas and Sheikh, Yaser},
	urldate = {2024-04-22},
	date = {2019-08-31},
	langid = {english},
	file = {Submitted Version:/Users/eduard.von-briesen/Zotero/storage/7RJX77YF/Lombardi et al. - 2019 - Neural volumes learning dynamic renderable volume.pdf:application/pdf},
}

@article{penner_soft_2017,
	title = {Soft 3D reconstruction for view synthesis},
	volume = {36},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3130800.3130855},
	doi = {10.1145/3130800.3130855},
	abstract = {We present a novel algorithm for view synthesis that utilizes a soft 3D reconstruction to improve quality, continuity and robustness. Our main contribution is the formulation of a soft 3D representation that preserves depth uncertainty through each stage of 3D reconstruction and rendering. We show that this representation is beneficial throughout the view synthesis pipeline. During view synthesis, it provides a soft model of scene geometry that provides continuity across synthesized views and robustness to depth uncertainty. During 3D reconstruction, the same robust estimates of scene visibility can be applied iteratively to improve depth estimation around object edges. Our algorithm is based entirely on O(1) filters, making it conducive to acceleration and it works with structured or unstructured sets of input views. We compare with recent classical and learning-based algorithms on plenoptic lightfields, wide baseline captures, and lightfield videos produced from camera arrays.},
	pages = {1--11},
	number = {6},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Penner, Eric and Zhang, Li},
	urldate = {2024-04-22},
	date = {2017-12-31},
	langid = {english},
}

@article{tao_zhou_stereo_2018,
	title = {Stereo Magnification: Learning View Synthesis using Multiplane Images},
	doi = {10.1145/3197517.3201323},
	abstract = {The view synthesis problem--generating novel views of a scene from known imagery--has garnered recent attention due in part to compelling applications in virtual and augmented reality. In this paper, we explore an intriguing scenario for view synthesis: extrapolating views from imagery captured by narrow-baseline stereo cameras, including {VR} cameras and now-widespread dual-lens camera phones. We call this problem stereo magnification, and propose a learning framework that leverages a new layered representation that we call multiplane images ({MPIs}). Our method also uses a massive new data source for learning view extrapolation: online videos on {YouTube}. Using data mined from such videos, we train a deep network that predicts an {MPI} from an input stereo image pair. This inferred {MPI} can then be used to synthesize a range of novel views of the scene, including views that extrapolate significantly beyond the input baseline. We show that our method compares favorably with several recent view synthesis methods, and demonstrate applications in magnifying narrow-baseline stereo images.},
	journaltitle = {{arXiv}: Computer Vision and Pattern Recognition},
	shortjournal = {{arXiv}: Computer Vision and Pattern Recognition},
	author = {{Tao Zhou} and {Tinghui Zhou} and {Richard Tucker} and {Richard Tucker} and {John P. Flynn} and {John Flynn} and {John Flynn} and {Graham Fyffe} and {Graham Fyffe} and {Noah Snavely} and {Noah Snavely}},
	date = {2018},
}

@article{g_r_riegler_free_2020,
	title = {Free View Synthesis},
	doi = {10.1007/978-3-030-58529-7_37},
	abstract = {We present a method for novel view synthesis from input images that are freely distributed around a scene. Our method does not rely on a regular arrangement of input views, can synthesize images for free camera movement through the scene, and works for general scenes with unconstrained geometric layouts. We calibrate the input images via {SfM} and erect a coarse geometric scaffold via {MVS}. This scaffold is used to create a proxy depth map for a novel view of the scene. Based on this depth map, a recurrent encoder-decoder network processes reprojected features from nearby views and synthesizes the new view. Our network does not need to be optimized for a given scene. After training on a dataset, it works in previously unseen environments with no fine-tuning or per-scene optimization. We evaluate the presented approach on challenging real-world datasets, including Tanks and Temples, where we demonstrate successful view synthesis for the first time and substantially outperform prior and concurrent work.},
	journaltitle = {European Conference on Computer Vision},
	shortjournal = {European Conference on Computer Vision},
	author = {{G. R. Riegler} and {Gernot Riegler} and {Vladlen Koltun} and {Vladlen Koltun}},
	date = {2020},
}

@article{pratul_p_srinivasan_pushing_2019,
	title = {Pushing the Boundaries of View Extrapolation With Multiplane Images},
	doi = {10.1109/cvpr.2019.00026},
	abstract = {We explore the problem of view synthesis from a narrow baseline pair of images, and focus on generating high-quality view extrapolations with plausible disocclusions. Our method builds upon prior work in predicting a multiplane image ({MPI}), which represents scene content as a set of {RGBA} planes within a reference view frustum and renders novel views by projecting this content into the target viewpoints. We present a theoretical analysis showing how the range of views that can be rendered from an {MPI} increases linearly with the {MPI} disparity sampling frequency, as well as a novel {MPI} prediction procedure that theoretically enables view extrapolations of up to 4 times the lateral viewpoint movement allowed by prior work. Our method ameliorates two specific issues that limit the range of views renderable by prior methods: 1) We expand the range of novel views that can be rendered without depth discretization artifacts by using a 3D convolutional network architecture along with a randomized-resolution training procedure to allow our model to predict {MPIs} with increased disparity sampling frequency. 2) We reduce the repeated texture artifacts seen in disocclusions by enforcing a constraint that the appearance of hidden content at any depth must be drawn from visible content at or behind that depth.},
	author = {{Pratul P. Srinivasan} and {Pratul P. Srinivasan} and {Richard Tucker} and {Richard Tucker} and {Jonathan T. Barron} and {Jonathan T. Barron} and {Ravi Ramamoorthi} and {Ravi Ramamoorthi} and {Ren Ng} and {Ren Ng} and {Noah Snavely} and {Noah Snavely}},
	date = {2019},
}

@article{inchang_choi_extreme_2019,
	title = {Extreme View Synthesis},
	doi = {10.1109/iccv.2019.00787},
	abstract = {We present Extreme View Synthesis, a solution for novel view extrapolation that works even when the number of input images is small---as few as two. In this context, occlusions and depth uncertainty are two of the most pressing issues, and worsen as the degree of extrapolation increases. We follow the traditional paradigm of performing depth-based warping and refinement, with a few key improvements. First, we estimate a depth probability volume, rather than just a single depth value for each pixel of the novel view. This allows us to leverage depth uncertainty in challenging regions, such as depth discontinuities. After using it to get an initial estimate of the novel view, we explicitly combine learned image priors and the depth uncertainty to synthesize a refined image with less artifacts. Our method is the first to show visually pleasing results for baseline magnifications of up to 30x.},
	author = {{Inchang Choi} and {Inchang Choi} and {Orazio Gallo} and {Orazio Gallo} and {Alejandro Troccoli} and {Alejandro Troccoli} and {Min H. Kim} and {Min H. Kim} and {Jan Kautz} and {Jan Kautz} and {Jan Kautz} and {Jan Kautz}},
	date = {2019},
}

@article{peter_hedman_deep_2019,
	title = {Deep blending for free-viewpoint image-based rendering},
	doi = {10.1145/3272127.3275084},
	abstract = {Free-viewpoint image-based rendering ({IBR}) is a standing challenge. {IBR} methods combine warped versions of input photos to synthesize a novel view. The image quality of this combination is directly affected by geometric inaccuracies of multi-view stereo ({MVS}) reconstruction and by view- and image-dependent effects that produce artifacts when contributions from different input views are blended. We present a new deep learning approach to blending for {IBR}, in which we use held-out real image data to learn blending weights to combine input photo contributions. Our Deep Blending method requires us to address several challenges to achieve our goal of interactive free-viewpoint {IBR} navigation. We first need to provide sufficiently accurate geometry so the Convolutional Neural Network ({CNN}) can succeed in finding correct blending weights. We do this by combining two different {MVS} reconstructions with complementary accuracy vs. completeness tradeoffs. To tightly integrate learning in an interactive {IBR} system, we need to adapt our rendering algorithm to produce a fixed number of input layers that can then be blended by the {CNN}. We generate training data with a variety of captured scenes, using each input photo as ground truth in a held-out approach. We also design the network architecture and the training loss to provide high quality novel view synthesis, while reducing temporal flickering artifacts. Our results demonstrate free-viewpoint {IBR} in a wide variety of scenes, clearly surpassing previous methods in visual quality, especially when moving far from the input cameras.},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Transactions on Graphics},
	author = {{Peter Hedman} and {Peter Hedman} and {Julien Philip} and {Julien Philip} and {True Price} and {True Price} and {Jan‐Michael Frahm} and {Jan-Michael Frahm} and {George Drettakis} and {George Drettakis} and {George Drettakis} and {Gabriel J. Brostow} and {Gabriel J. Brostow}},
	date = {2019},
}

@article{maxim_tatarchenko_single-view_2015,
	title = {Single-view to Multi-view: Reconstructing Unseen Views with a Convolutional Network},
	abstract = {We present a convolutional network capable of generating images of a previously unseen object from arbitrary viewpoints given a single image of this object. The input to the network is a single image and the desired new viewpoint; the output is a view of the object from this desired viewpoint. The network is trained on renderings of synthetic 3D models. It learns an implicit 3D representation of the object class, which allows it to transfer shape knowledge from training instances to a new object instance. Beside the color image, the network can also generate the depth map of an object from arbitrary viewpoints. This allows us to predict 3D point clouds from a single image, which can be fused into a surface mesh. We experimented with cars and chairs. Even though the network is trained on artificial data, it generalizes well to objects in natural images without any modifications.},
	journaltitle = {{arXiv}: Computer Vision and Pattern Recognition},
	shortjournal = {{arXiv}: Computer Vision and Pattern Recognition},
	author = {{Maxim Tatarchenko} and {Maxim Tatarchenko} and {Alexey Dosovitskiy} and {Alexey Dosovitskiy} and {Thomas Brox} and {Thomas Brox}},
	date = {2015},
}

@online{noauthor_polycam_nodate,
	title = {Polycam - {LiDAR} \& 3D Scanner for {iPhone} \& Android},
	url = {https://poly.cam/},
	abstract = {Capture reality with Polycam's {LiDAR} scanner \& photogrammetry. Make 3D scans and download 3D models. Available on {iPhone}, {iPad}, Android and Web.},
	urldate = {2024-04-23},
	file = {Snapshot:/Users/eduard.von-briesen/Zotero/storage/QQB636SD/poly.cam.html:text/html},
}

@online{noauthor_record3d_nodate,
	title = {Record3D — 3D Videos and Point Cloud ({RGBD}) Streaming for {iOS}},
	url = {https://record3d.app/},
	urldate = {2024-04-23},
	file = {Record3D — 3D Videos and Point Cloud (RGBD) Streaming for iOS:/Users/eduard.von-briesen/Zotero/storage/A2P5CV4F/record3d.app.html:text/html},
}

@online{noauthor_nerfacto_nodate,
	title = {Nerfacto},
	url = {http://docs.nerf.studio/nerfology/methods/nerfacto.html},
	abstract = {Running the Method: We provide a few additional variants:,,,, Method, Description, Memory, Speed,,, nerfacto, Default Model,{\textasciitilde}6GB, Fast,, nerfacto-big, Larger higher quality,{\textasciitilde}12GB, Slower,, nerfacto...},
	urldate = {2024-04-25},
	langid = {english},
	file = {Snapshot:/Users/eduard.von-briesen/Zotero/storage/IBDND32R/nerfacto.html:text/html},
}

@article{jan-niklas_dihlmann_signerf_2024,
	title = {{SIGNeRF}: Scene Integrated Generation for Neural Radiance Fields},
	doi = {10.48550/arxiv.2401.01647},
	abstract = {Advances in image diffusion models have recently led to notable improvements in the generation of high-quality images. In combination with Neural Radiance Fields ({NeRFs}), they enabled new opportunities in 3D generation. However, most generative 3D approaches are object-centric and applying them to editing existing photorealistic scenes is not trivial. We propose {SIGNeRF}, a novel approach for fast and controllable {NeRF} scene editing and scene-integrated object generation. A new generative update strategy ensures 3D consistency across the edited images, without requiring iterative optimization. We find that depth-conditioned diffusion models inherently possess the capability to generate 3D consistent views by requesting a grid of images instead of single views. Based on these insights, we introduce a multi-view reference sheet of modified images. Our method updates an image collection consistently based on the reference sheet and refines the original {NeRF} with the newly generated image set in one go. By exploiting the depth conditioning mechanism of the image diffusion model, we gain fine control over the spatial location of the edit and enforce shape guidance by a selected region or an external mesh.},
	journaltitle = {{arXiv}.org},
	shortjournal = {{arXiv}.org},
	author = {{Jan-Niklas Dihlmann} and {Andreas Engelhardt} and {Hendrik P. A. Lensch}},
	date = {2024},
}

@online{ai_luma_nodate,
	title = {Luma {AI}},
	url = {https://lumalabs.ai/},
	abstract = {Building visual {AI} to expand human imagination and capabilities},
	titleaddon = {Luma {AI}},
	author = {{AI}, Luma},
	urldate = {2024-04-25},
	langid = {english},
	file = {Snapshot:/Users/eduard.von-briesen/Zotero/storage/HUVGP3KH/lumalabs.ai.html:text/html},
}

@software{noauthor_tensorflowtensorboard_2024,
	title = {tensorflow/tensorboard},
	rights = {Apache-2.0},
	url = {https://github.com/tensorflow/tensorboard},
	abstract = {{TensorFlow}'s Visualization Toolkit},
	publisher = {tensorflow},
	urldate = {2024-04-25},
	date = {2024-04-24},
	note = {original-date: 2017-05-15T20:08:07Z},
}

@online{noauthor_unreal_nodate,
	title = {Unreal Engine},
	url = {https://www.unrealengine.com/en-US/home},
	abstract = {Whatever your vision, bring it to life with Unreal Engine: the world's most advanced real-time 3D creation tool. Join our community of developers and get started today.},
	titleaddon = {Unreal Engine},
	urldate = {2024-04-25},
	langid = {american},
}

@online{noauthor_blender_nodate,
	title = {Blender - Free and Open 3D Creation Software},
	url = {https://www.blender.org/},
	urldate = {2024-04-25},
	file = {blender.org - Home of the Blender project - Free and Open 3D Creation Software:/Users/eduard.von-briesen/Zotero/storage/HXWC66W7/www.blender.org.html:text/html},
}

@online{noauthor_docker_2022,
	title = {Docker: Accelerated Container Application Development},
	url = {https://www.docker.com/},
	shorttitle = {Docker},
	abstract = {Docker is a platform designed to help developers build, share, and run container applications. We handle the tedious setup, so you can focus on the code.},
	urldate = {2024-04-25},
	date = {2022-05-10},
	langid = {american},
}

@online{noauthor_express_nodate,
	title = {Express - Node.js-Framework von Webanwendungen},
	url = {https://expressjs.com/de/},
	urldate = {2024-04-25},
	langid = {german},
	file = {Snapshot:/Users/eduard.von-briesen/Zotero/storage/NJPIYM8S/de.html:text/html},
}

@online{noauthor_sosci_nodate,
	title = {{SoSci} Survey ‣ the Professional Solution for Your Online Survey},
	url = {https://www.soscisurvey.de/},
	urldate = {2024-04-26},
	file = {SoSci Survey ‣ the Professional Solution for Your Online Survey:/Users/eduard.von-briesen/Zotero/storage/QNESGCSA/www.soscisurvey.de.html:text/html},
}

@online{noauthor_volinga_nodate,
	title = {Volinga},
	url = {https://volinga.ai/},
	urldate = {2024-04-29},
	file = {Volinga Creator:/Users/eduard.von-briesen/Zotero/storage/DPYJ767I/volinga.ai.html:text/html},
}
