
@software{noauthor_automatic1111_2022,
	title = {{AUTOMATIC}1111 - Stable Diffusion Web {UI}},
	rights = {{AGPL}-3.0},
	url = {https://github.com/AUTOMATIC1111/stable-diffusion-webui},
	abstract = {Stable Diffusion web {UI}},
	urldate = {2023-10-16},
	date = {2022-08},
	note = {original-date: 2022-08-22T14:05:26Z},
}

@online{team_gradio_nodate,
	title = {Gradio},
	url = {https://gradio.app},
	abstract = {Build \& Share Delightful Machine Learning Apps},
	author = {Team, Gradio},
	urldate = {2023-10-16},
	langid = {german},
	file = {Snapshot:/Users/eduard.von-briesen/Zotero/storage/N2SXMK6R/www.gradio.app.html:text/html},
}

@inproceedings{wang_clip-nerf_2022,
	title = {{CLIP}-{NeRF}: Text-and-Image Driven Manipulation of Neural Radiance Fields},
	pages = {3835--3844},
	booktitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Wang, Can and Chai, Menglei and He, Mingming and Chen, Dongdong and Liao, Jing},
	date = {2022-06},
	file = {Wang et al. - 2022 - CLIP-NeRF Text-and-Image Driven Manipulation of N.pdf:/Users/eduard.von-briesen/Zotero/storage/QSE9GGYZ/Wang et al. - 2022 - CLIP-NeRF Text-and-Image Driven Manipulation of N.pdf:application/pdf},
}

@misc{bar-tal_text2live_2022,
	title = {Text2LIVE: Text-Driven Layered Image and Video Editing},
	author = {Bar-Tal, Omer and Ofri-Amar, Dolev and Fridman, Rafail and Kasten, Yoni and Dekel, Tali},
	date = {2022},
	note = {\_eprint: 2204.02491},
	file = {Bar-Tal et al. - 2022 - Text2LIVE Text-Driven Layered Image and Video Edi.pdf:/Users/eduard.von-briesen/Zotero/storage/I3G8Z7US/Bar-Tal et al. - 2022 - Text2LIVE Text-Driven Layered Image and Video Edi.pdf:application/pdf},
}

@article{muller_instant_2022,
	title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
	volume = {41},
	url = {https://doi.org/10.1145/3528223.3530127},
	doi = {10.1145/3528223.3530127},
	pages = {102:1--102:15},
	number = {4},
	journaltitle = {{ACM} Trans. Graph.},
	author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
	date = {2022-07},
	note = {Place: New York, {NY}, {USA}
Publisher: {ACM}},
	file = {Müller et al. - 2022 - Instant Neural Graphics Primitives with a Multires.pdf:/Users/eduard.von-briesen/Zotero/storage/NP73B3FX/Müller et al. - 2022 - Instant Neural Graphics Primitives with a Multires.pdf:application/pdf},
}

@inproceedings{bao_sine_2023,
	title = {{SINE}: Semantic-driven Image-based {NeRF} Editing with Prior-guided Editing Field},
	booktitle = {The {IEEE}/{CVF} Computer Vision and Pattern Recognition Conference ({CVPR})},
	author = {Bao, Chong and Zhang, Yinda and Yang, Bangbang and Fan, Tianxing and Yang, Zesong and Bao, Hujun and Zhang, Guofeng and Cui, Zhaopeng},
	date = {2023},
	file = {Bao et al. - 2023 - SINE Semantic-driven Image-based NeRF Editing wit.pdf:/Users/eduard.von-briesen/Zotero/storage/WTEP74IW/Bao et al. - 2023 - SINE Semantic-driven Image-based NeRF Editing wit.pdf:application/pdf},
}

@misc{wu_palettenerf_2022,
	title = {{PaletteNeRF}: Palette-based Color Editing for {NeRFs}},
	author = {Wu, Qiling and Tan, Jianchao and Xu, Kun},
	date = {2022},
	note = {\_eprint: 2212.12871},
	file = {Wu et al. - 2022 - PaletteNeRF Palette-based Color Editing for NeRFs.pdf:/Users/eduard.von-briesen/Zotero/storage/B75YG3CP/Wu et al. - 2022 - PaletteNeRF Palette-based Color Editing for NeRFs.pdf:application/pdf},
}

@inproceedings{haque_instruct-nerf2nerf_2023,
	title = {Instruct-{NeRF}2NeRF: Editing 3D Scenes with Instructions},
	booktitle = {Proceedings of the {IEEE}/{CVF} International Conference on Computer Vision},
	author = {Haque, Ayaan and Tancik, Matthew and Efros, Alexei and Holynski, Aleksander and Kanazawa, Angjoo},
	date = {2023},
	file = {Haque et al. - 2023 - Instruct-NeRF2NeRF Editing 3D Scenes with Instruc.pdf:/Users/eduard.von-briesen/Zotero/storage/UABJT6NX/Haque et al. - 2023 - Instruct-NeRF2NeRF Editing 3D Scenes with Instruc.pdf:application/pdf},
}

@inproceedings{yuan_nerf-editing_2022,
	title = {{NeRF}-Editing: Geometry Editing of Neural Radiance Fields},
	pages = {18353--18364},
	booktitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Yuan, Yu-Jie and Sun, Yang-Tian and Lai, Yu-Kun and Ma, Yuewen and Jia, Rongfei and Gao, Lin},
	date = {2022-06},
	file = {Yuan et al. - 2022 - NeRF-Editing Geometry Editing of Neural Radiance .pdf:/Users/eduard.von-briesen/Zotero/storage/WXK4HSYZ/Yuan et al. - 2022 - NeRF-Editing Geometry Editing of Neural Radiance .pdf:application/pdf},
}

@inproceedings{tancik_nerfstudio_2023,
	title = {Nerfstudio: A Modular Framework for Neural Radiance Field Development},
	series = {{SIGGRAPH} '23},
	booktitle = {{ACM} {SIGGRAPH} 2023 Conference Proceedings},
	author = {Tancik, Matthew and Weber, Ethan and Ng, Evonne and Li, Ruilong and Yi, Brent and Kerr, Justin and Wang, Terrance and Kristoffersen, Alexander and Austin, Jake and Salahi, Kamyar and Ahuja, Abhik and {McAllister}, David and Kanazawa, Angjoo},
	date = {2023},
	file = {Tancik et al. - 2023 - Nerfstudio A Modular Framework for Neural Radianc.pdf:/Users/eduard.von-briesen/Zotero/storage/495RZA9X/Tancik et al. - 2023 - Nerfstudio A Modular Framework for Neural Radianc.pdf:application/pdf},
}

@inproceedings{laugwitz_construction_2008,
	location = {Berlin, Heidelberg},
	title = {Construction and Evaluation of a User Experience Questionnaire},
	isbn = {978-3-540-89350-9},
	doi = {10.1007/978-3-540-89350-9_6},
	series = {Lecture Notes in Computer Science},
	abstract = {An end-user questionnaire to measure user experience quickly in a simple and immediate way while covering a preferably comprehensive impression of the product user experience was the goal of the reported construction process. An empirical approach for the item selection was used to ensure practical relevance of items. Usability experts collected terms and statements on user experience and usability, including ‘hard’ as well as ‘soft’ aspects. These statements were consolidated and transformed into a first questionnaire version containing 80 bipolar items. It was used to measure the user experience of software products in several empirical studies. Data were subjected to a factor analysis which resulted in the construction of a 26 item questionnaire including the six factors Attractiveness, Perspicuity, Efficiency, Dependability, Stimulation, and Novelty. Studies conducted for the original German questionnaire and an English version indicate a satisfactory level of reliability and construct validity.},
	pages = {63--76},
	booktitle = {{HCI} and Usability for Education and Work},
	publisher = {Springer},
	author = {Laugwitz, Bettina and Held, Theo and Schrepp, Martin},
	editor = {Holzinger, Andreas},
	date = {2008},
	langid = {english},
	keywords = {Perceived usability, Questionnaire, Software evaluation, Usability assessment, User experience, User satisfaction},
}

@online{noauthor_trpc_nodate,
	title = {{tRPC} - Move Fast and Break Nothing. End-to-end typesafe {APIs} made easy. {\textbar} {tRPC}},
	url = {https://trpc.io/},
	urldate = {2024-03-25},
	file = {tRPC - Move Fast and Break Nothing. End-to-end typesafe APIs made easy. | tRPC:/Users/eduard.von-briesen/Zotero/storage/QUK4WCCG/trpc.io.html:text/html},
}

@online{noauthor_vite_nodate,
	title = {Vite},
	url = {https://vitejs.dev},
	abstract = {Next Generation Frontend Tooling},
	urldate = {2024-03-25},
	langid = {american},
	file = {Snapshot:/Users/eduard.von-briesen/Zotero/storage/GWUZBRHW/vitejs.dev.html:text/html},
}

@online{noauthor_react_nodate,
	title = {React},
	url = {https://react.dev/},
	abstract = {React is the library for web and native user interfaces. Build user interfaces out of individual pieces called components written in {JavaScript}. React is designed to let you seamlessly combine components written by independent people, teams, and organizations.},
	urldate = {2024-03-25},
	langid = {english},
	file = {Snapshot:/Users/eduard.von-briesen/Zotero/storage/7EKI6DWW/react.dev.html:text/html},
}

@online{noauthor_tailwind_2020,
	title = {Tailwind {CSS} - Rapidly build modern websites without ever leaving your {HTML}.},
	url = {https://tailwindcss.com/},
	abstract = {Tailwind {CSS} is a utility-first {CSS} framework for rapidly building modern websites without ever leaving your {HTML}.},
	urldate = {2024-03-25},
	date = {2020-11-15},
	langid = {english},
}

@online{noauthor_daisyui_nodate,
	title = {{daisyUI} — Tailwind {CSS} Components ( version 4 update is here )},
	url = {https://daisyui.com/},
	abstract = {Best Tailwind Components Library - Free {UI} components for Tailwind {CSS}},
	urldate = {2024-03-25},
	langid = {english},
	file = {Snapshot:/Users/eduard.von-briesen/Zotero/storage/GA2YJUU7/daisyui.com.html:text/html},
}

@software{noauthor_nerfstudio-projectviser_2024,
	title = {nerfstudio-project/viser},
	rights = {Apache-2.0},
	url = {https://github.com/nerfstudio-project/viser},
	abstract = {Web-based 3D visualization + Python},
	publisher = {nerfstudio},
	urldate = {2024-03-25},
	date = {2024-03-25},
	note = {original-date: 2022-11-16T00:13:51Z},
	keywords = {python, visualization, web},
}

@software{yi_brentyityro_2024,
	title = {brentyi/tyro},
	rights = {{MIT}},
	url = {https://github.com/brentyi/tyro},
	abstract = {Zero-effort {CLI} interfaces \& config objects, from types},
	author = {Yi, Brent},
	urldate = {2024-03-31},
	date = {2024-03-26},
	note = {original-date: 2021-10-05T08:54:08Z},
	keywords = {argparse, argument-parsing, dataclasses},
}

@article{mildenhall_nerf_2021,
	title = {{NeRF}: representing scenes as neural radiance fields for view synthesis},
	volume = {65},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3503250},
	doi = {10.1145/3503250},
	shorttitle = {{NeRF}},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, ϕ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.},
	pages = {99--106},
	number = {1},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	urldate = {2024-04-08},
	date = {2021-12-17},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/GM44WRYH/Mildenhall et al. - 2021 - NeRF representing scenes as neural radiance field.pdf:application/pdf},
}

@article{feng_meshnet_2019,
	title = {{MeshNet}: Mesh Neural Network for 3D Shape Representation},
	volume = {33},
	rights = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4840},
	doi = {10.1609/aaai.v33i01.33018279},
	shorttitle = {{MeshNet}},
	abstract = {Mesh is an important and powerful type of data for 3D shapes and widely studied in the field of computer vision and computer graphics. Regarding the task of 3D shape representation, there have been extensive research efforts concentrating on how to represent 3D shapes well using volumetric grid, multi-view and point cloud. However, there is little effort on using mesh data in recent years, due to the complexity and irregularity of mesh data. In this paper, we propose a mesh neural network, named {MeshNet}, to learn 3D shape representation from mesh data. In this method, face-unit and feature splitting are introduced, and a general architecture with available and effective blocks are proposed. In this way, {MeshNet} is able to solve the complexity and irregularity problem of mesh and conduct 3D shape representation well. We have applied the proposed {MeshNet} method in the applications of 3D shape classification and retrieval. Experimental results and comparisons with the state-of-the-art methods demonstrate that the proposed {MeshNet} can achieve satisfying 3D shape classification and retrieval performance, which indicates the effectiveness of the proposed method on 3D shape representation.},
	pages = {8279--8286},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Feng, Yutong and Feng, Yifan and You, Haoxuan and Zhao, Xibin and Gao, Yue},
	urldate = {2024-04-18},
	date = {2019-07-17},
	langid = {english},
	note = {Number: 01},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/CC2CG95W/Feng et al. - 2019 - MeshNet Mesh Neural Network for 3D Shape Represen.pdf:application/pdf},
}

@inproceedings{curless_volumetric_1996,
	location = {New York, {NY}, {USA}},
	title = {A volumetric method for building complex models from range images},
	isbn = {978-0-89791-746-9},
	url = {https://dl.acm.org/doi/10.1145/237170.237269},
	doi = {10.1145/237170.237269},
	series = {{SIGGRAPH} '96},
	abstract = {A number of techniques have been developed for reconstructing surfaces by integrating groups of aligned range images. A desirable set of properties for such algorithms includes: incremental updating, representation of directional uncertainty, the ability to fill gaps in the reconstruction, and robustness in the presence of outliers. Prior algorithms possess subsets of these properties. In this paper, we present a volumetric method for integrating range images that possesses all of these properties. Our volumetric representation consists of a cumulative weighted signed distance function. Working with one range image at a time, we first scan-convert it to a distance function, then combine this with the data already acquired using a simple additive scheme. To achieve space efficiency, we employ a run-length encoding of the volume. To achieve time efficiency, we resample the range image to align with the voxel grid and traverse the range and voxel scanlines synchronously. We generate the final manifold by extracting an isosurface from the volumetric grid. We show that under certain assumptions, this isosurface is optimal in the least squares sense. To fill gaps in the model, we tessellate over the boundaries between regions seen to be empty and regions never observed. Using this method, we are able to integrate a large number of range images (as many as 70) yielding seamless, high-detail models of up to 2.6 million triangles.},
	pages = {303--312},
	booktitle = {Proceedings of the 23rd annual conference on Computer graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Curless, Brian and Levoy, Marc},
	urldate = {2024-04-18},
	date = {1996-08-01},
	keywords = {isosurface extraction, range image integration, surface fitting, three-dimensional shape recovery},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/LZBMG6LE/Curless and Levoy - 1996 - A volumetric method for building complex models fr.pdf:application/pdf},
}

@inproceedings{levoy_light_1996,
	location = {New York, {NY}, {USA}},
	title = {Light field rendering},
	isbn = {978-0-89791-746-9},
	url = {https://dl.acm.org/doi/10.1145/237170.237199},
	doi = {10.1145/237170.237199},
	series = {{SIGGRAPH} '96},
	abstract = {A number of techniques have been proposed for flying through scenes by redisplaying previously rendered or digitized views. Techniques have also been proposed for interpolating between views by warping input images, using depth information or correspondences between multiple images. In this paper, we describe a simple and robust method for generating new views from arbitrary camera positions without depth information or feature matching, simply by combining and resampling the available images. The key to this technique lies in interpreting the input images as 2D slices of a 4D function - the light field. This function completely characterizes the flow of light through unobstructed space in a static scene with fixed illumination. We describe a sampled representation for light fields that allows for both efficient creation and display of inward and outward looking views. We have created light fields from large arrays of both rendered and digitized images. The latter are acquired using a video camera mounted on a computer-controlled gantry. Once a light field has been created, new views may be constructed in real time by extracting slices in appropriate directions. Since the success of the method depends on having a high sample rate, we describe a compression system that is able to compress the light fields we have generated by more than a factor of 100:1 with very little loss of fidelity. We also address the issues of antialiasing during creation, and resampling during slice extraction.},
	pages = {31--42},
	booktitle = {Proceedings of the 23rd annual conference on Computer graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Levoy, Marc and Hanrahan, Pat},
	urldate = {2024-04-19},
	date = {1996-08-01},
	keywords = {epipolar analysis, holographic stereogram, image-based rendering, light field, vector quantization},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/9W52MK9V/Levoy and Hanrahan - 1996 - Light field rendering.pdf:application/pdf},
}

@inproceedings{gortler_lumigraph_1996,
	location = {New York, {NY}, {USA}},
	title = {The lumigraph},
	isbn = {978-0-89791-746-9},
	url = {https://dl.acm.org/doi/10.1145/237170.237200},
	doi = {10.1145/237170.237200},
	series = {{SIGGRAPH} '96},
	abstract = {This paper discusses a new method for capturing the complete appearance of both synthetic and real world objects and scenes, representing this information, and then using this representation to render images of the object from new camera positions. Unlike the shape capture process traditionally used in computer vision and the rendering process traditionally used in computer graphics, our approach does not rely on geometric representations. Instead we sample and reconstruct a 4D function, which we call a Lumigraph. The Lumigraph is a subsetof the complete plenoptic function that describes the flow of light at all positions in all directions. With the Lumigraph, new images of the object can be generated very quickly, independent of the geometric or illumination complexity of the scene or object. The paper discusses a complete working system including the capture of samples, the construction of the Lumigraph, and the subsequent rendering of images from this new representation.},
	pages = {43--54},
	booktitle = {Proceedings of the 23rd annual conference on Computer graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Gortler, Steven J. and Grzeszczuk, Radek and Szeliski, Richard and Cohen, Michael F.},
	urldate = {2024-04-19},
	date = {1996-08-01},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/ZNGGKBML/Gortler et al. - 1996 - The lumigraph.pdf:application/pdf},
}

@article{zitnick_high-quality_2004,
	title = {High-quality video view interpolation using a layered representation},
	volume = {23},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/1015706.1015766},
	doi = {10.1145/1015706.1015766},
	abstract = {The ability to interactively control viewpoint while watching a video is an exciting application of image-based rendering. The goal of our work is to render dynamic scenes with interactive viewpoint control using a relatively small number of video cameras. In this paper, we show how high-quality video-based rendering of dynamic scenes can be accomplished using multiple synchronized video streams combined with novel image-based modeling and rendering algorithms. Once these video streams have been processed, we can synthesize any intermediate view between cameras at any time, with the potential for space-time manipulation.In our approach, we first use a novel color segmentation-based stereo algorithm to generate high-quality photoconsistent correspondences across all camera views. Mattes for areas near depth discontinuities are then automatically extracted to reduce artifacts during view synthesis. Finally, a novel temporal two-layer compressed representation that handles matting is developed for rendering at interactive rates.},
	pages = {600--608},
	number = {3},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Zitnick, C. Lawrence and Kang, Sing Bing and Uyttendaele, Matthew and Winder, Simon and Szeliski, Richard},
	urldate = {2024-04-19},
	date = {2004-08-01},
	keywords = {Computer Vision, Dynamic Scenes, Image-Based Rendering},
}

@inproceedings{debevec_modeling_1996,
	location = {New York, {NY}, {USA}},
	title = {Modeling and rendering architecture from photographs: a hybrid geometry- and image-based approach},
	isbn = {978-0-89791-746-9},
	url = {https://dl.acm.org/doi/10.1145/237170.237191},
	doi = {10.1145/237170.237191},
	series = {{SIGGRAPH} '96},
	shorttitle = {Modeling and rendering architecture from photographs},
	abstract = {We present a new approach for modeling and rendering existing architectural scenes from a sparse set of still photographs. Our modeling approach, which combines both geometry-based and image-based techniques, has two components. The first component is a photogrammetric modeling method which facilitates the recovery of the basic geometry of the photographed scene. Our photogrammetric modeling approach is effective, convenient, and robust because it exploits the constraints that are characteristic of architectural scenes. The second component is a model-based stereo algorithm, which recovers how the real scene deviates from the basic model. By making use of the model, our stereo technique robustly recovers accurate depth from widely-spaced image pairs. Consequently, our approach can model large architectural environments with far fewer photographs than current image-based modeling approaches. For producing renderings, we present view-dependent texture mapping, a method of compositing multiple views of a scene that better simulates geometric detail on basic models. Our approach can be used to recover models for use in either geometry-based or image-based rendering systems. We present results that demonstrate our approach's ability to create realistic renderings of architectural scenes from viewpoints far from the original photographs.},
	pages = {11--20},
	booktitle = {Proceedings of the 23rd annual conference on Computer graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Debevec, Paul E. and Taylor, Camillo J. and Malik, Jitendra},
	urldate = {2024-04-19},
	date = {1996-08-01},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/EP6GGLC3/Debevec et al. - 1996 - Modeling and rendering architecture from photograp.pdf:application/pdf},
}

@inproceedings{buehler_unstructured_2001,
	location = {New York, {NY}, {USA}},
	title = {Unstructured lumigraph rendering},
	isbn = {978-1-58113-374-5},
	url = {https://dl.acm.org/doi/10.1145/383259.383309},
	doi = {10.1145/383259.383309},
	series = {{SIGGRAPH} '01},
	abstract = {We describe an image based rendering approach that generalizes many current image based rendering algorithms, including light field rendering and view-dependent texture mapping. In particular, it allows for lumigraph-style rendering from a set of input cameras in arbitrary configurations (i.e., not restricted to a plane or to any specific manifold). In the case of regular and planar input camera positions, our algorithm reduces to a typical lumigraph approach. When presented with fewer cameras and good approximate geometry, our algorithm behaves like view-dependent texture mapping. The algorithm achieves this flexibility because it is designed to meet a set of specific goals that we describe. We demonstrate this flexibility with a variety of examples.},
	pages = {425--432},
	booktitle = {Proceedings of the 28th annual conference on Computer graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Buehler, Chris and Bosse, Michael and {McMillan}, Leonard and Gortler, Steven and Cohen, Michael},
	urldate = {2024-04-19},
	date = {2001-08-01},
	keywords = {image-based rendering},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/M7T5W9U9/Buehler et al. - 2001 - Unstructured lumigraph rendering.pdf:application/pdf},
}

@article{seitz_photorealistic_1999,
	title = {Photorealistic Scene Reconstruction by Voxel Coloring},
	volume = {35},
	issn = {1573-1405},
	url = {https://doi.org/10.1023/A:1008176507526},
	doi = {10.1023/A:1008176507526},
	abstract = {A novel scene reconstruction technique is presented, different from previous approaches in its ability to cope with large changes in visibility and its modeling of intrinsic scene color and texture information. The method avoids image correspondence problems by working in a discretized scene space whose voxels are traversed in a fixed visibility ordering. This strategy takes full account of occlusions and allows the input cameras to be far apart and widely distributed about the environment. The algorithm identifies a special set of invariant voxels which together form a spatial and photometric reconstruction of the scene, fully consistent with the input images. The approach is evaluated with images from both inward-facing and outward-facing cameras.},
	pages = {151--173},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {International Journal of Computer Vision},
	author = {Seitz, Steven M. and Dyer, Charles R.},
	urldate = {2024-04-19},
	date = {1999-11-01},
	langid = {english},
	keywords = {image correspondence, invariants, multi-baseline stereo, occlusion, photorealism, scene reconstruction, voxel representations},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/95DE3SLT/Seitz and Dyer - 1999 - Photorealistic Scene Reconstruction by Voxel Color.pdf:application/pdf},
}

@inproceedings{chen_view_1993,
	location = {New York, {NY}, {USA}},
	title = {View interpolation for image synthesis},
	isbn = {978-0-89791-601-1},
	url = {https://dl.acm.org/doi/10.1145/166117.166153},
	doi = {10.1145/166117.166153},
	series = {{SIGGRAPH} '93},
	abstract = {Image-space simplifications have been used to accelerate the calculation of computer graphic images since the dawn of visual simulation. Texture mapping has been used to provide a means by which images may themselves be used as display primitives. The work reported by this paper endeavors to carry this concept to its logical extreme by using interpolated images to portray three-dimensional scenes. The special-effects technique of morphing, which combines interpolation of texture maps and their shape, is applied to computing arbitrary intermediate frames from an array of prestored images. If the images are a structured set of views of a 3D object or scene, intermediate frames derived by morphing can be used to approximate intermediate 3D transformations of the object or scene. Using the view interpolation approach to synthesize 3D scenes has two main advantages. First, the 3D representation of the scene may be replaced with images. Second, the image synthesis time is independent of the scene complexity. The correspondence between images, required for the morphing method, can be predetermined automatically using the range data associated with the images. The method is further accelerated by a quadtree decomposition and a view-independent visible priority. Our experiments have shown that the morphing can be performed at interactive rates on today?s high-end personal computers. Potential applications of the method include virtual holograms, a walkthrough in a virtual environment, image-based primitives and incremental rendering. The method also can be used to greatly accelerate the computation of motion blur and soft shadows cast by area light sources.},
	pages = {279--288},
	booktitle = {Proceedings of the 20th annual conference on Computer graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Chen, Shenchang Eric and Williams, Lance},
	urldate = {2024-04-19},
	date = {1993-09-01},
	keywords = {image morphing, incremental rendering, interpolation, motion blur, motion compensation, real-time display, shadow, virtual holography, virtual reality},
	file = {Full Text PDF:/Users/eduard.von-briesen/Zotero/storage/SAPYXF6R/Chen and Williams - 1993 - View interpolation for image synthesis.pdf:application/pdf},
}

@inproceedings{flynn_deep_2016,
	title = {Deep Stereo: Learning to Predict New Views from the World's Imagery},
	url = {https://ieeexplore.ieee.org/document/7780964},
	doi = {10.1109/CVPR.2016.595},
	shorttitle = {Deep Stereo},
	abstract = {Deep networks have recently enjoyed enormous success when applied to recognition and classification problems in computer vision [22, 33], but their use in graphics problems has been limited ([23, 7] are notable recent exceptions). In this work, we present a novel deep architecture that performs new view synthesis directly from pixels, trained from a large number of posed image sets. In contrast to traditional approaches, which consist of multiple complex stages of processing, each of which requires careful tuning and can fail in unexpected ways, our system is trained end-to-end. The pixels from neighboring views of a scene are presented to the network, which then directly produces the pixels of the unseen view. The benefits of our approach include generality (we only require posed image sets and can easily apply our method to different domains), and high quality results on traditionally difficult scenes. We believe this is due to the end-to-end nature of our system, which is able to plausibly generate pixels according to color, depth, and texture priors learnt automatically from the training data. We show view interpolation results on imagery from the {KITTI} dataset [12], from data from [1] as well as on Google Street View images. To our knowledge, our work is the first to apply deep learning to the problem of new view synthesis from sets of real-world, natural imagery.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {5515--5524},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Flynn, John and Neulander, Ivan and Philbin, James and Snavely, Noah},
	urldate = {2024-04-19},
	date = {2016-06},
	note = {{ISSN}: 1063-6919},
	keywords = {Google, Image color analysis, Poles and towers, Shape, Three-dimensional displays, Training, Training data},
	file = {Submitted Version:/Users/eduard.von-briesen/Zotero/storage/2XZPKBIB/Flynn et al. - 2016 - Deep Stereo Learning to Predict New Views from th.pdf:application/pdf},
}
